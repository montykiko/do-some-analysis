{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:40:59.821944Z",
     "start_time": "2018-03-28T10:40:59.817933Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:35:39.634555Z",
     "start_time": "2018-03-28T10:35:39.584403Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"D:/data_files/train_u6lujuX_CVtuZ9i.csv\",index_col = \"Loan_ID\",engine='python')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:36:14.399517Z",
     "start_time": "2018-03-28T10:36:14.376456Z"
    }
   },
   "outputs": [],
   "source": [
    "data.loc[(data['Gender']=='Female')&(data['Education']=='Not Graduate') & (data['Loan_Status']==\"Y\"),[\"Gender\",\"Education\",\"Loan_Status\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:37:36.496803Z",
     "start_time": "2018-03-28T10:37:36.372282Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create a new function:\n",
    "def num_missing(x):\n",
    "    return sum(x.isnull())\n",
    "\n",
    "#Applying per column:\n",
    "print(\"Missing values per column:\")\n",
    "print(data.apply(num_missing,axis=0))   #axis=0 defines that function is to be applied on each column\n",
    "\n",
    "#Applying per row:\n",
    "print(\"Missing values per row:\")\n",
    "print(data.apply(num_missing,axis=1).head()) #axis=1 defines that function is to be applied on each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing missing files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘fillna()’ does it in one go. It is used for updating missing values with the overall mean/mode/median of the column. Let’s impute the ‘Gender’, ‘Married’ and ‘Self_Employed’ columns with their respective modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:40:07.014581Z",
     "start_time": "2018-03-28T10:40:06.992522Z"
    }
   },
   "outputs": [],
   "source": [
    "#first we import a function to determine the mode\n",
    "from scipy.stats import mode\n",
    "print(mode(data['Gender'].dropna()))\n",
    "print(mode(data['Gender'].dropna()).mode[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：原文中，mode(data['Gender'])会报错，指出scipy的mode函数无法处理列表中包含混合类型的情况，比方说上面的例子就是包含了缺失值NAN类型和字符串类型，所以无法直接处理。\n",
    "\n",
    "同时也指出Pandas自带的mode函数是可以处理混合类型的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:41:07.685780Z",
     "start_time": "2018-03-28T10:41:07.674753Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "Series.mode(data['Gender'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:41:10.051277Z",
     "start_time": "2018-03-28T10:41:10.024206Z"
    }
   },
   "outputs": [],
   "source": [
    "#Impute the values:\n",
    "data['Gender'].fillna(Series.mode(data['Gender'])[0],inplace=True)\n",
    "data['Married'].fillna(Series.mode(data['Married'])[0],inplace=True)\n",
    "data['Self_Employed'].fillna(Series.mode(data['Self_Employed'])[0],inplace=True)\n",
    "\n",
    "#Now check the missing values again to confirm:\n",
    "print(data.apply(num_missing,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Determine pivot table\n",
    "impute_grps = data.pivot_table(values=[\"LoanAmount\"],index=[\"Gender\",\"Married\",\"Self_Employed\"],aggfunc=np.mean)\n",
    "print(impute_grps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate only through rows with missing LoanAmount\n",
    "for i,row in data.loc[data['LoanAmount'].isnull(),:].iterrows():\n",
    "    ind = tuple([row['Gender'],row['Married'],row['Self_Employed']])\n",
    "    data.loc[i,'LoanAmount'] = impute_grps.loc[ind].values[0]\n",
    "    \n",
    "#Now check the missing values again to confirm:\n",
    "print(data.apply(num_missing,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to get an initial “feel” (view) of the data. Here, we can validate some basic hypothesis. For instance, in this case, “Credit_History” is expected to affect the loan status significantly. This can be tested using cross-tabulation as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data[\"Credit_History\"],data['Loan_Status'],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are absolute numbers. But, percentages can be more intuitive in making some quick insights. We can do this using the apply function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:45:38.590681Z",
     "start_time": "2018-03-28T10:45:38.533530Z"
    }
   },
   "outputs": [],
   "source": [
    "def perConvert(ser):\n",
    "    return ser/float(ser[-1])\n",
    "\n",
    "pd.crosstab(data['Credit_History'],data['Loan_Status'],margins=True).apply(perConvert,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging dataframes become essential when we have information coming from different sources to be collated. Consider a hypothetical case where the average property rates (INR per sq meters) is available for different property types. Let’s define a dataframe as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_rates = pd.DataFrame([1000, 5000, 12000], index=['Rural','Semiurban','Urban'],columns=['rates'])\n",
    "prop_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = data.merge(right=prop_rates,how='inner',left_on='Property_Area',right_index=True,sort=False)\n",
    "data_merged.pivot_table(values='Credit_History',index=['Property_Area','rates'],aggfunc=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = data.sort_values(['ApplicantIncome','CoapplicantIncome'], ascending=False)\n",
    "data_sorted[['ApplicantIncome','CoapplicantIncome']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting (Boxplot & Histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data.boxplot(column=\"ApplicantIncome\",by=\"Loan_Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(column=\"ApplicantIncome\",by=\"Loan_Status\",bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut function for binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes numerical values make more sense if clustered together. For example, if we’re trying to model traffic (#cars on road) with time of the day (minutes). The exact minute of an hour might not be that relevant for predicting traffic as compared to actual period of the day like “Morning”, “Afternoon”, “Evening”, “Night”, “Late Night”. Modeling traffic this way will be more intuitive and will avoid overfitting.\n",
    "\n",
    "Here we define a simple function which can be re-used for binning any variable fairly easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning:\n",
    "def binning(col,cut_points,labels=None):\n",
    "    #Define min and max values:\n",
    "    minval = col.min()\n",
    "    maxval = col.max()\n",
    "    \n",
    "    #create list by adding min and max to cut_points\n",
    "    break_points = [minval] + cut_points + [maxval]\n",
    "    \n",
    "    #if no labels provided, use default labels 0...n-1\n",
    "    if not labels:\n",
    "        labels = range(len(cut_points)+1)\n",
    "    #Binning using cut function of pandas\n",
    "    colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n",
    "    return colBin\n",
    "\n",
    "#Bining age:\n",
    "cut_points=[90,140,190]\n",
    "labels =['low','medium','high','very high']\n",
    "data['LoanAmount_Bin'] = binning(data['LoanAmount'],cut_points,labels)\n",
    "print(pd.value_counts(data['LoanAmount_Bin'],sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding nominal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we find a case where we’ve to modify the categories of a nominal variable. This can be due to various reasons:\n",
    "\n",
    "1.Some algorithms (like Logistic Regression) require all inputs to be numeric. So nominal variables are mostly coded as 0, 1….(n-1)\n",
    "\n",
    "2.Sometimes a category might be represented in 2 ways. For e.g. temperature might be recorded as “High”, “Medium”, “Low”, “H”, “low”. Here, both “High” and “H” refer to same category. Similarly, in “Low” and “low” there is only a difference of case. But, python would read them as different levels.\n",
    "\n",
    "3.Some categories might have very low frequencies and its generally a good idea to combine them.\n",
    "Here I’ve defined a generic function which takes in input as a dictionary and codes the values using ‘replace’ function in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a generic function using Pandas replace function\n",
    "def coding(col,codeDict):\n",
    "    colCoded = pd.Series(col,copy=True)\n",
    "    for key, value in codeDict.items():\n",
    "        colCoded.replace(key,value,inplace=True)\n",
    "    return colCoded\n",
    "\n",
    "#Coding LoanStatus as Y=1,N=0:\n",
    "print('Before Coindg:')\n",
    "print(pd.value_counts(data['Loan_Status']))\n",
    "data['Loan_Status_Coded'] = coding(data['Loan_Status'],{'N':0,'Y':1})\n",
    "print('\\nAfter Coding:')\n",
    "print(pd.value_counts(data['Loan_Status_Coded']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating over rows of a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a frequently used operation. Still, you don’t want to get stuck. Right? At times you may need to iterate through all rows using a for loop. For instance, one common problem we face is the incorrect treatment of variables in Python. This generally happens when:\n",
    "\n",
    "1.Nominal variables with numeric categories are treated as numerical.\n",
    "2.Numeric variables with characters entered in one of the rows (due to a data error) are considered categorical.\n",
    "\n",
    "So it’s generally a good idea to manually define the column types. If we check the data types of all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that Credit_History is a nominal variable but appearing as float. A good way to tackle such issues is to create a csv file with column names and types. This way, we can make a generic function to read the file and assign column data types. For instance, here I have created a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T10:56:03.645405Z",
     "start_time": "2018-03-28T10:56:03.624349Z"
    }
   },
   "outputs": [],
   "source": [
    "colTypes = pd.read_csv(r'D:/data_files/datatypes.csv',engine='python')\n",
    "print(colTypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading this file, we can iterate through each row and assign the datatype using column ‘type’ to the variable name defined in the ‘feature’ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through each row and assign variable type.\n",
    "#Note: astype is used to assign types\n",
    "\n",
    "for i, row in colTypes.iterrows():  #i: dataframe index; row: each row in series format\n",
    "    if row['type']==\"categorical\":\n",
    "        data[row['feature']]=data[row['feature']].astype(np.object)\n",
    "    elif row['type']==\"continuous\":\n",
    "        data[row['feature']]=data[row['feature']].astype(np.float)\n",
    "print (data.dtypes)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
